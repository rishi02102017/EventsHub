{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ccb1b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load JSON data\n",
    "with open('data/events.json', 'r') as f:\n",
    "    events = json.load(f)\n",
    "\n",
    "with open('data/registrations.json', 'r') as f:\n",
    "    registrations = json.load(f)\n",
    "\n",
    "with open('data/users.json', 'r') as f:\n",
    "    users = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ca4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59149d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_past_events(user_id):\n",
    "    \"\"\"Returns all events the user has registered for, regardless of status\"\"\"\n",
    "    past_event_ids = [r['eventId'] for r in registrations if r['userId'] == user_id]\n",
    "    return [e for e in events if e['_id'] in past_event_ids]\n",
    "\n",
    "def filter_candidate_events(user_id, include_attended_events=True):\n",
    "\n",
    "    current_time = datetime.now(timezone.utc)\n",
    "    candidates = []\n",
    "    \n",
    "\n",
    "    registered_event_ids = {r['eventId'] for r in registrations if r['userId'] == user_id}\n",
    "    \n",
    "    for event in events:\n",
    "        # Parse event times (timezone-aware)\n",
    "        end_time = datetime.fromisoformat(event['endTime'].replace('+00:00', '+00:00')).replace(tzinfo=timezone.utc)\n",
    "        is_past = end_time <= current_time\n",
    "        is_registered = event['_id'] in registered_event_ids\n",
    "        \n",
    "        \n",
    "        if is_registered:\n",
    "    \n",
    "            include_event = include_attended_events and is_past\n",
    "        else:\n",
    "            include_event = (event['status'] == 'live') and (not is_past or include_attended_events)\n",
    "        if include_event:\n",
    "            event_copy = event.copy()\n",
    "            event_copy.update({\n",
    "                'is_past': is_past,\n",
    "                'is_registered': is_registered,\n",
    "                'user_attended': is_registered and is_past\n",
    "            })\n",
    "            candidates.append(event_copy)\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f1ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "def extract_features_for_clustering(events):\n",
    "    # Text features (TF-IDF)\n",
    "    descriptions = [e[\"description\"] for e in events]\n",
    "    tfidf = TfidfVectorizer(max_features=50, stop_words=\"english\")  \n",
    "    tfidf_features = tfidf.fit_transform(descriptions).toarray()  \n",
    "    \n",
    "    event_types = [e[\"eventType\"] for e in events]\n",
    "    organizers = [e[\"organizerId\"] for e in events]\n",
    "    encoder = OneHotEncoder()\n",
    "    \n",
    "    combined_categorical = list(zip(event_types, organizers))\n",
    "    categorical_features = encoder.fit_transform(combined_categorical).toarray()\n",
    "    \n",
    "    # Numeric features (Days until start)\n",
    "    current_time = datetime.now(timezone.utc)\n",
    "    days_until_start = [\n",
    "        (datetime.fromisoformat(e[\"startTime\"].replace(\"+00:00\", \"+00:00\")) - current_time).days\n",
    "        for e in events\n",
    "    ]\n",
    "    days_features = np.array(days_until_start).reshape(-1, 1)  # Shape: (n_events, 1)\n",
    "    \n",
    "    \n",
    "    features = np.hstack([tfidf_features, categorical_features, days_features])\n",
    "    return features\n",
    "\n",
    "def cluster(events, n_clusters=5):\n",
    "    descriptions = [e[\"description\"] for e in events]\n",
    "    tfidf = TfidfVectorizer(max_features=50, stop_words=\"english\")\n",
    "    tfidf_features = tfidf.fit_transform(descriptions).toarray()\n",
    "    \n",
    "    event_types = [e[\"eventType\"] for e in events]\n",
    "    organizers = [e[\"organizerId\"] for e in events]\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    categorical_features = encoder.fit_transform(np.column_stack([event_types, organizers])).toarray()\n",
    "    \n",
    "\n",
    "    features = np.hstack([tfidf_features, categorical_features])\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(features)\n",
    "    for i, event in enumerate(events):\n",
    "        event[\"cluster\"] = int(clusters[i])\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ffa152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_events(user_id,events, top_n=3):\n",
    "    clustered_events = cluster(events)\n",
    "    past_events = get_user_past_events(user_id)\n",
    "    user_clusters = list(set([e[\"cluster\"] for e in past_events])) if past_events else []\n",
    "    \n",
    "    candidate_events = filter_candidate_events(user_id)\n",
    "    \n",
    "\n",
    "    if candidate_events:\n",
    "        event_descriptions = [e[\"description\"] for e in candidate_events]\n",
    "        past_descriptions = [e[\"description\"] for e in past_events] if past_events else [\"\"]\n",
    "        \n",
    "        tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "        tfidf_matrix = tfidf.fit_transform(event_descriptions + past_descriptions)\n",
    "        candidate_vectors = tfidf_matrix[:len(candidate_events)]\n",
    "        past_vectors = tfidf_matrix[len(candidate_events):]\n",
    "        \n",
    "        similarity_scores = cosine_similarity(past_vectors, candidate_vectors).mean(axis=0) if past_events else [0.5]*len(candidate_events)\n",
    "        \n",
    "        scored_events = []\n",
    "        for idx, event in enumerate(candidate_events):\n",
    "            cluster_score = 1 if user_clusters and event[\"cluster\"] in user_clusters else 0.5\n",
    "            type_score = 1 if past_events and event[\"eventType\"] in [e[\"eventType\"] for e in past_events] else 0.5\n",
    "            \n",
    "            start_time = datetime.fromisoformat(event[\"startTime\"].replace(\"+00:00\", \"+00:00\")).replace(tzinfo=timezone.utc)\n",
    "            days_until_start = (start_time - datetime.now(timezone.utc)).days\n",
    "            time_score = 1 / (1 + abs(days_until_start))\n",
    "            \n",
    "            score = (0.4 * similarity_scores[idx] + \n",
    "                    0.3 * cluster_score + \n",
    "                    0.2 * type_score + \n",
    "                    0.1 * time_score)\n",
    "            \n",
    "            scored_events.append((event, score))\n",
    "\n",
    "        print(scored_events[0][1])\n",
    "\n",
    "        scored_events.sort(key=lambda x: x[1], reverse=True)\n",
    "        recommendations = [event for event, _ in scored_events[:top_n]]\n",
    "        if recommendations:\n",
    "            return recommendations\n",
    "    \n",
    "    event_popularity = defaultdict(int)\n",
    "    for reg in registrations:\n",
    "        event_popularity[reg[\"eventId\"]] += 1\n",
    "    \n",
    "    user_registered_ids = {r[\"eventId\"] for r in registrations if r[\"userId\"] == user_id}\n",
    "    \n",
    "    if user_clusters:\n",
    "        cluster_events = [e for e in clustered_events \n",
    "                         if e[\"cluster\"] in user_clusters\n",
    "                         and e[\"_id\"] not in user_registered_ids]\n",
    "        cluster_events.sort(key=lambda x: -event_popularity[x[\"_id\"]])\n",
    "        if cluster_events:\n",
    "            return cluster_events[:top_n]\n",
    "    \n",
    "    all_events = [e for e in clustered_events if e[\"_id\"] not in user_registered_ids]\n",
    "    all_events.sort(key=lambda x: -event_popularity[x[\"_id\"]])\n",
    "    return all_events[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6ea1b64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3569191076890392\n",
      "Recommended Events:\n",
      "- Alvarez, Foster and Walker (concert, starts 2025-04-20T00:00:00.000Z)\n",
      "- Espinoza-Lane (virtual, starts 2025-04-14T00:00:00.000Z)\n",
      "- Jackson LLC (marathon, starts 2025-01-06T00:00:00.000Z)\n"
     ]
    }
   ],
   "source": [
    "user_id = \"681e535ec99231a537cc9c88\"  # Dr. Beth Koch\n",
    "recommended_events = recommend_events(user_id, events, top_n=3)\n",
    "\n",
    "print(\"Recommended Events:\")\n",
    "for event in recommended_events:\n",
    "    print(f\"- {event['title']} ({event['eventType']}, starts {event['startTime']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "06c64c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def recommend_events(user_id, top_n=3):\n",
    "    # Step 1: Try to recommend live/upcoming events first\n",
    "    past_events = get_user_past_events(user_id)\n",
    "    candidate_events = filter_candidate_events(user_id)  # Live/upcoming events\n",
    "    \n",
    "    if candidate_events:\n",
    "        # Content-based filtering (same as before)\n",
    "        event_descriptions = [e['description'] for e in candidate_events]\n",
    "        past_descriptions = [e['description'] for e in past_events]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(event_descriptions + past_descriptions)\n",
    "        candidate_vectors = tfidf_matrix[:len(candidate_events)]\n",
    "        past_vectors = tfidf_matrix[len(candidate_events):]\n",
    "        \n",
    "        similarity_scores = cosine_similarity(past_vectors, candidate_vectors).mean(axis=0)\n",
    "        \n",
    "        scored_events = []\n",
    "        for idx, event in enumerate(candidate_events):\n",
    "            start_time = datetime.fromisoformat(event['startTime'].replace('+00:00', '+00:00')).replace(tzinfo=timezone.utc)\n",
    "            days_until_start = (start_time - datetime.now(timezone.utc)).days\n",
    "            \n",
    "            score = (\n",
    "                0.5 * similarity_scores[idx] +  # Text similarity\n",
    "                0.3 * (1 if event['eventType'] in [e['eventType'] for e in past_events] else 0) +  # Type match\n",
    "                0.2 * (1 / (1 + days_until_start))  # Time relevance\n",
    "            )\n",
    "            scored_events.append((event, score))\n",
    "        \n",
    "        scored_events.sort(key=lambda x: x[1], reverse=True)\n",
    "        recommended = [event for event, _ in scored_events[:top_n]]\n",
    "        \n",
    "        if recommended:  # Return if we found matches\n",
    "            return recommended\n",
    "    \n",
    "    # Step 2: Fallback to ANY popular events (including past ones)\n",
    "    event_popularity = defaultdict(int)\n",
    "    for reg in registrations:\n",
    "        event_popularity[reg['eventId']] += 1\n",
    "    \n",
    "    # Sort all events by popularity (regardless of status/time)\n",
    "    all_events_sorted = sorted(\n",
    "        events,\n",
    "        key=lambda x: (-event_popularity[x['_id']], x['title']), \n",
    "    )\n",
    "    \n",
    "    # Exclude events the user already registered for\n",
    "    user_registered_ids = {r['eventId'] for r in registrations if r['userId'] == user_id}\n",
    "    fallback_events = [\n",
    "        e for e in all_events_sorted\n",
    "        if e['_id'] not in user_registered_ids\n",
    "    ]\n",
    "    \n",
    "    return fallback_events[:top_n]  # Return top-N popular events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dbb393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4d7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a2d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8defc08c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c5764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d9672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee088d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee7f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26cc741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8bf6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7732/3384045893.py:11: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/tmp/ipykernel_7732/3384045893.py:14: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
      "/tmp/ipykernel_7732/3384045893.py:38: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  summary = conversation.run(summary_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Message: Hey, did anyone watch the keynote?\n",
      "\n",
      "--- Live Summary ---\n",
      "We've just started our conversation, and so far, you've asked if anyone watched the keynote, but we haven't discussed any details about it yet. You then asked me to summarize our current chat, which I'm happy to do. Our conversation is still in its early stages, and we're just getting started.\n",
      "--------------------\n",
      "\n",
      "New Message: Yes! The AI section was amazing!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m chat_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     49\u001b[0m summary_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 51\u001b[0m chat_thread\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m     52\u001b[0m summary_thread\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/threading.py:1147\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/threading.py:1167\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[1;32m   1168\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Message: I liked the demo on real-time voice translation.\n",
      "New Message: What was the name of the presenter again?\n",
      "\n",
      "--- Live Summary ---\n",
      "We've been discussing the keynote, specifically the AI section, which you thought was amazing. You mentioned enjoying the demo on real-time voice translation, but unfortunately, I don't have information on who the presenter was, as that detail wasn't mentioned in our conversation. Our chat has been focused on your positive experience with the keynote's AI segment, but we haven't explored other topics yet.\n",
      "--------------------\n",
      "\n",
      "New Message: I think it was Dr. Lisa Wong.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Exception in threading.excepthook:\n",
      "Exception ignored in thread started by: <bound method Thread._bootstrap of <Thread(Thread-4 (stream_chat), stopped 135876694857408)>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/threading.py\", line 1030, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self._invoke_excepthook(self)\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/threading.py\", line 1389, in invoke_excepthook\n",
      "    local_print(\"Exception in threading.excepthook:\",\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/ipykernel/iostream.py\", line 573, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/ipykernel/iostream.py\", line 266, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/zmq/sugar/socket.py\", line 696, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 742, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 783, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 138, in zmq.backend.cython.socket._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/ipykernel/iostream.py\", line 573, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/ipykernel/iostream.py\", line 266, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/zmq/sugar/socket.py\", line 696, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 742, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 783, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 138, in zmq.backend.cython.socket._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "Exception in thread Thread-5 (summarize_chat):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_7732/3384045893.py\", line 38, in summarize_chat\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 191, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 606, in run\n",
      "    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 191, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/langchain/chains/base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 946, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 765, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 1011, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/langchain_groq/chat_models.py\", line 480, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 322, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/groq/_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/groq/_base_client.py\", line 958, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/aldrax/anaconda3/lib/python3.12/site-packages/groq/_base_client.py\", line 1061, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jn1j78k0ex7sr2vk46c8jfse` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98697, Requested 1613. Please try again in 4m27.801999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Message: Yeah, and she mentioned something about a new open-source project.\n",
      "--- Live Summary ---\n",
      "We've been discussing the keynote, particularly the AI section, which you found amazing and featured a demo on real-time voice translation that you enjoyed. You recalled that the presenter was Dr. Lisa Wong, and she mentioned a new open-source project, although we didn't delve into the specifics of the project. Our conversation has centered around your positive experience with the keynote's AI segment and the details you remembered about Dr. Wong's presentation.\n",
      "--------------------\n",
      "\n",
      "\n",
      "--- Live Summary ---\n",
      "We've been discussing the keynote, specifically the AI section presented by Dr. Lisa Wong, which you thought was amazing and featured a notable demo on real-time voice translation. Dr. Wong also mentioned a new open-source project during her presentation, although we haven't discussed the details of the project. Our conversation has been focused on your experience with the keynote's AI segment and the information you remembered about Dr. Wong's talk.\n",
      "--------------------\n",
      "\n",
      "\n",
      "--- Live Summary ---\n",
      "We've been discussing the keynote, specifically the AI section presented by Dr. Lisa Wong, which you thought was amazing and featured a notable demo on real-time voice translation. Dr. Wong also mentioned a new open-source project during her presentation, although we haven't discussed the details of the project. Our conversation has been focused on your experience with the keynote's AI segment and the information you remembered about Dr. Wong's talk, with no new topics or details added recently.\n",
      "--------------------\n",
      "\n",
      "\n",
      "--- Live Summary ---\n",
      "We've been discussing the keynote, specifically the AI section presented by Dr. Lisa Wong, which you thought was amazing and featured a notable demo on real-time voice translation. Dr. Wong also mentioned a new open-source project during her presentation, although we haven't discussed the details of the project. Our conversation has been focused on your experience with the keynote's AI segment and the information you remembered about Dr. Wong's talk, with no new topics or details added recently, and we've been summarizing the same points in our last few exchanges.\n",
      "--------------------\n",
      "\n",
      "\n",
      "--- Live Summary ---\n",
      "We've been discussing the keynote, specifically the AI section presented by Dr. Lisa Wong, which you thought was amazing and featured a notable demo on real-time voice translation. Dr. Wong also mentioned a new open-source project during her presentation, although we haven't discussed the details of the project. Our conversation has been focused on your experience with the keynote's AI segment and the information you remembered about Dr. Wong's talk, with multiple summaries of the same points in our recent exchanges, and no new information or topics have been introduced.\n",
      "--------------------\n",
      "\n",
      "\n",
      "--- Live Summary ---\n",
      "We've been discussing the keynote, specifically the AI section presented by Dr. Lisa Wong, which you thought was amazing and featured a notable demo on real-time voice translation. Dr. Wong also mentioned a new open-source project during her presentation, although we haven't discussed the details of the project. Our conversation has been focused on your experience with the keynote's AI segment and the information you remembered about Dr. Wong's talk, with multiple summaries of the same points in our recent exchanges, indicating that our conversation has reached a point where we're reiterating the same information without introducing new topics or details.\n",
      "--------------------\n",
      "\n",
      "\n",
      "--- Live Summary ---\n",
      "We've been discussing the keynote, specifically the AI section presented by Dr. Lisa Wong, which you thought was amazing and featured a notable demo on real-time voice translation. Dr. Wong also mentioned a new open-source project during her presentation, although we haven't discussed the details of the project. Our conversation has been focused on your experience with the keynote's AI segment and the information you remembered about Dr. Wong's talk, with multiple summaries of the same points in our recent exchanges, suggesting that we may be ready to move on to a new topic or explore the open-source project in more detail.\n",
      "--------------------\n",
      "\n",
      "\n",
      "--- Live Summary ---\n",
      "We've been discussing the keynote, specifically the AI section presented by Dr. Lisa Wong, which you thought was amazing and featured a notable demo on real-time voice translation. Dr. Wong also mentioned a new open-source project during her presentation, although we haven't discussed the details of the project, and our conversation has been centered around your experience with the keynote's AI segment. Our recent exchanges have consisted of repeated summaries of the same points, indicating that we may be ready to transition to a new topic or delve deeper into the open-source project that Dr. Wong mentioned.\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_groq import ChatGroq\n",
    "from threading import Thread\n",
    "\n",
    "# === Setup Groq LLM ===\n",
    "llm = ChatGroq(temperature=0.2, model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "# === Memory for storing messages ===\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# === LangChain conversation chain ===\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
    "\n",
    "# === Simulated live chat stream ===\n",
    "chat_stream = [\n",
    "    \"Hey, did anyone watch the keynote?\",\n",
    "    \"Yes! The AI section was amazing!\",\n",
    "    \"I liked the demo on real-time voice translation.\",\n",
    "    \"What was the name of the presenter again?\",\n",
    "    \"I think it was Dr. Lisa Wong.\",\n",
    "    \"Yeah, and she mentioned something about a new open-source project.\",\n",
    "    \"Exactly, it’s called WhisperX!\"\n",
    "]\n",
    "\n",
    "# === Function to simulate chat ingestion ===\n",
    "def stream_chat():\n",
    "    for msg in chat_stream:\n",
    "        memory.chat_memory.add_user_message(msg)\n",
    "        print(f\"New Message: {msg}\")\n",
    "        time.sleep(2)  # Simulate delay between messages\n",
    "\n",
    "# === Function to summarize chat every few seconds ===\n",
    "def summarize_chat():\n",
    "    while True:\n",
    "        summary_prompt = \"Summarize the current chat so far in 2-3 sentences.\"\n",
    "        summary = conversation.run(summary_prompt)\n",
    "        print(\"\\n--- Live Summary ---\")\n",
    "        print(summary)\n",
    "        print(\"--------------------\\n\")\n",
    "        time.sleep(6)  # Summarize every 6 seconds\n",
    "\n",
    "# === Start threads for streaming and summarizing ===\n",
    "chat_thread = Thread(target=stream_chat)\n",
    "summary_thread = Thread(target=summarize_chat)\n",
    "\n",
    "chat_thread.start()\n",
    "summary_thread.start()\n",
    "\n",
    "chat_thread.join()\n",
    "summary_thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac966e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello. How can I help you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hello\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e5a08a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary for Event ID 681e535fc99231a537cc9c90 ===\n",
      "\n",
      "Here is a summary of the chat:\n",
      "\n",
      "The chat discussion revolved around the concept of change\" and its potential benefits. The conversation started with a mention of a \"computer inside\" which seemed to be related to an energy project. The project's goal was to bring about positive impact, with one of the key benefits being the project could lead to personal growth and development for the individuals involved.\n",
      "\n",
      "The chat also touched on the idea that this project had the potential to bring about significant change, with one participant mentioning that the project could be a \"game-changer\". Although the details of the project were not fully fleshed out, the participants were enthusiastic about its potential to drive positive change.\n",
      "\n",
      "Overall, the chat was optimistic in tone, with participants expressing excitement about the project's potential to make a meaningful impact. Unfortunately, the chat did not delve deeper into the specifics of the project, leaving some questions unanswered.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# === Load chat history from JSON and filter by eventId ===\n",
    "def load_chat_messages(filepath: str, event_id: str) -> str:\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Filter messages by eventId\n",
    "    messages = [entry[\"message\"] for entry in data if entry[\"eventId\"] == event_id]\n",
    "\n",
    "    # Combine into a single string\n",
    "    return \"\\n\".join(messages)\n",
    "\n",
    "# === Setup LLM ===\n",
    "llm = ChatGroq(temperature=0.3, model_name=\"llama3-70b-8192\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat\"],\n",
    "    template=\"\"\"\n",
    "You are an assistant summarizing an event's chat history. Write a 2–3 paragraph summary that captures key ideas, names, and discussions.\n",
    "\n",
    "Chat:\n",
    "{chat}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# === Run summarization ===\n",
    "event_id = \"681e535fc99231a537cc9c90\"  # Replace with your target eventId\n",
    "chat_text = load_chat_messages(\"chat_history.json\", event_id)\n",
    "summary = chain.run(chat=chat_text)\n",
    "\n",
    "print(\"\\n=== Summary for Event ID\", event_id, \"===\\n\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b700b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b40205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f966de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
